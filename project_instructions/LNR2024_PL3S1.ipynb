{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfu5rYyDl8oM",
        "outputId": "e5a43b28-91db-4ac0-d50d-67cd2b35a719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pan-clef-2024-oppositional'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 80 (delta 34), reused 72 (delta 26), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (80/80), 3.71 MiB | 24.83 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n"
          ]
        }
      ],
      "source": [
        "#Installing the utilities provided by the organizers of the shared task \"\"\n",
        "!git clone https://github.com/dkorenci/pan-clef-2024-oppositional.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/pan-clef-2024-oppositional/generate_requirements.sh"
      ],
      "metadata": {
        "id": "qdj112eImJzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "451f159b-8ff3-4d4b-e426-06056d1effdb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipreqs\n",
            "  Downloading pipreqs-0.5.0-py3-none-any.whl (33 kB)\n",
            "Collecting docopt==0.6.2 (from pipreqs)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ipython==8.12.3 (from pipreqs)\n",
            "  Downloading ipython-8.12.3-py3-none-any.whl (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.3/798.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbconvert<8.0.0,>=7.11.0 (from pipreqs)\n",
            "  Downloading nbconvert-7.16.3-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.4/257.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarg==0.1.9 (from pipreqs)\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (4.4.2)\n",
            "Collecting jedi>=0.16 (from ipython==8.12.3->pipreqs)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (0.1.7)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (2.16.1)\n",
            "Collecting stack-data (from ipython==8.12.3->pipreqs)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==8.12.3->pipreqs) (4.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from yarg==0.1.9->pipreqs) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (3.1.3)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (2.1.5)\n",
            "Collecting mistune<4,>=2.0.3 (from nbconvert<8.0.0,>=7.11.0->pipreqs)\n",
            "  Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.10.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (24.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.2.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==8.12.3->pipreqs) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.2.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.19.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==8.12.3->pipreqs) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.12.3->pipreqs) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->yarg==0.1.9->pipreqs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->yarg==0.1.9->pipreqs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->yarg==0.1.9->pipreqs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->yarg==0.1.9->pipreqs) (2024.2.2)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython==8.12.3->pipreqs)\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython==8.12.3->pipreqs)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting pure-eval (from stack-data->ipython==8.12.3->pipreqs)\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.18.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (23.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.3.3)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=837d1c52478e8b16c4062a035e4b0b5b7aec62c50d0ad38bc570edc5dc273b03\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pure-eval, docopt, mistune, jedi, executing, asttokens, yarg, stack-data, ipython, nbconvert, pipreqs\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.34.0\n",
            "    Uninstalling ipython-7.34.0:\n",
            "      Successfully uninstalled ipython-7.34.0\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 6.5.4\n",
            "    Uninstalling nbconvert-6.5.4:\n",
            "      Successfully uninstalled nbconvert-6.5.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asttokens-2.4.1 docopt-0.6.2 executing-2.0.1 ipython-8.12.3 jedi-0.19.1 mistune-3.0.2 nbconvert-7.16.3 pipreqs-0.5.0 pure-eval-0.2.2 stack-data-0.6.3 yarg-0.1.9\n",
            "INFO: Not scanning for jupyter notebooks.\n",
            "ERROR: Failed on file: ./drive/MyDrive/parsingACLFiles.py\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pipreqs\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pipreqs/pipreqs.py\", line 609, in main\n",
            "    init(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pipreqs/pipreqs.py\", line 533, in init\n",
            "    candidates = get_all_imports(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pipreqs/pipreqs.py\", line 153, in get_all_imports\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pipreqs/pipreqs.py\", line 139, in get_all_imports\n",
            "    tree = ast.parse(contents)\n",
            "  File \"/usr/lib/python3.10/ast.py\", line 50, in parse\n",
            "    return compile(source, filename, mode, flags,\n",
            "  File \"<unknown>\", line 103\n",
            "    print \"Parsing ...\", filename\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print(...)?\n",
            "mv: cannot stat 'reqs.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RULJcH1qRwn",
        "outputId": "2561c3b5-fb9b-4608-ec14-84ec99a9571a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->-r requirements.txt (line 1)) (2.2.1+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch]->-r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]->-r requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]->-r requirements.txt (line 1)) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]->-r requirements.txt (line 1)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->-r requirements.txt (line 1)) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]->-r requirements.txt (line 1)) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "sys.path.append(\"/content/pan-clef-2024-oppositional/\")\n",
        "TRAIN_DATASET_ES=\"/content/drive/MyDrive/LNR/LNR 2024/Dataset-Oppositionl/training/dataset_oppositional/dataset_es_train.json\"\n",
        "TRAIN_DATASET_EN=\"/content/drive/MyDrive/LNR/LNR 2024/Dataset-Oppositionl/training/dataset_oppositional/dataset_en_train.json\"\n",
        "TEST_DATASET_EN =\"/content/drive/MyDrive/LNR/LNR 2024/Dataset-Oppositionl/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\"\n",
        "TEST_DATASET_ES =\"/content/drive/MyDrive/LNR/LNR 2024/Dataset-Oppositionl/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\""
      ],
      "metadata": {
        "id": "LBnGu_lCqooY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data_tools.dataset_loaders import load_dataset_classification\n",
        "texts_es, labels_es, ids_es = load_dataset_classification(\"es\", string_labels=False, positive_class='conspiracy')\n",
        "texts_en, labels_en, ids_en = load_dataset_classification(\"en\", string_labels=False, positive_class='conspiracy')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FClav3aLqp7N",
        "outputId": "54e53a23-bd66-4a96-dbbe-17fecbd18cdb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading official JSON es dataset\n",
            "Loading official JSON en dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a toy representation for the training dataset in both languages.\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(analyzer = 'word', max_features=4000, lowercase=True)\n",
        "X_es = vectorizer.fit_transform(texts_es.tolist())\n",
        "vectorizer.get_feature_names_out()\n",
        "X_es= X_es.toarray()\n",
        "Y_es= np.asarray(labels_es.tolist())\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer = 'word', max_features=4000, lowercase=True)\n",
        "X_en = vectorizer.fit_transform(texts_en.tolist())\n",
        "vectorizer.get_feature_names_out()\n",
        "X_en= X_en.toarray()\n",
        "Y_en= np.asarray(labels_en.tolist())"
      ],
      "metadata": {
        "id": "x8jMKgbG8UtM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nFbeEK6g2fJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 1\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "# Here, we are dividing the trainig set in two subsets one for training and other for validation.\n",
        "# In practice, X_en and X_es represent the matrices with text representations from data provided, and Y_en and Y_es represent the corresponding labels of either critical or conspiracy texts.\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "#Using the default parameters\n",
        "clf_en = svm.SVC()\n",
        "clf_en.fit(X_en_train, y_en_train)\n",
        "predicted = clf_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "#Using the default parameters\n",
        "clf_es = svm.SVC()\n",
        "clf_es.fit(X_es_train, y_es_train)\n",
        "predicted = clf_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWVdBVtlqq5m",
        "outputId": "4eab4e32-6159-4193-9857-4c5ed05a39d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6158546335768069\n",
            "Predicted\t-->\t [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.5147027940361023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 2\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "#Using the default parameters\n",
        "clf_en = LogisticRegression()\n",
        "clf_en.fit(X_en_train, y_en_train)\n",
        "predicted = clf_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "\n",
        "#Using the default parameters\n",
        "clf_es= LogisticRegression()\n",
        "clf_es.fit(X_es_train, y_es_train)\n",
        "predicted = clf_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OljOO7hSrsef",
        "outputId": "09591fab-c6e0-4d18-e2ab-597d114b7f8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6900063416551955\n",
            "Predicted\t-->\t [0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.6607052811439216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 3\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "clf_en = DecisionTreeClassifier()\n",
        "clf_en.fit(X_en_train, y_en_train)\n",
        "predicted = clf_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "clf_es = DecisionTreeClassifier()\n",
        "clf_es.fit(X_es_train, y_es_train)\n",
        "predicted = clf_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxsE7xo9r39B",
        "outputId": "71db34fa-cc71-4716-c575-25325c1bc7a6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.5609754043385264\n",
            "Predicted\t-->\t [1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.39356106462973733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 4\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "#Using the default parameters\n",
        "clf_en = MLPClassifier(random_state=1, max_iter=300)\n",
        "clf_en.fit(X_en_train, y_en_train)\n",
        "predicted = clf_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "\n",
        "#Using the default parameters\n",
        "clf_es = MLPClassifier(random_state=1, max_iter=300)\n",
        "clf_es.fit(X_es_train, y_es_train)\n",
        "predicted = clf_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnUhlPNCsAYR",
        "outputId": "cba54e40-602d-486e-87a8-4dc32d24cc83"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6559441012749416\n",
            "Predicted\t-->\t [0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.6146430393212517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 5\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#Here you can define the training and test sets for the shared task\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "# create the base models, Logistic Regression, Decision Tree, and Support Vector Machine\n",
        "model1 = LogisticRegression()\n",
        "model2 = DecisionTreeClassifier()\n",
        "model3 = SVC()\n",
        "# create the voting ensemble\n",
        "ensemble_en = VotingClassifier(estimators=[('lr', model1), ('dt', model2), ('svm', model3)], voting='hard')\n",
        "# train the ensemble\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# Evaluate the ensemble\n",
        "predicted=ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "\n",
        "# create the base models, Logistic Regression, Decision Tree, and Support Vector Machine\n",
        "model1 = LogisticRegression()\n",
        "model2 = DecisionTreeClassifier()\n",
        "model3 = SVC()\n",
        "# create the voting ensemble\n",
        "ensemble_es = VotingClassifier(estimators=[('lr', model1), ('dt', model2), ('svm', model3)], voting='hard')\n",
        "# train the ensemble\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# Evaluate the ensemble\n",
        "predicted=ensemble_en.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ],
      "metadata": {
        "id": "sCoPxuxwsLPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef5ef2d7-6c5f-445c-c308-898a95856f6f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6898001532114117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.1695519914577131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5KeANyPsWX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 6\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#Here you can define the training and test sets for the shared task\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "ensemble_en =BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)\n",
        "# train the ensemble\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# evaluate the ensemble\n",
        "ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "ensemble_es =BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)\n",
        "# train the ensemble\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# evaluate the ensemble\n",
        "ensemble_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ],
      "metadata": {
        "id": "H1ooMhw7s-Co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561b8dff-3357-4054-aed8-42f3e4f2b81b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: -0.03485408693137697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.1695519914577131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 8\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#Here you can define the training and test sets for the shared task\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "ensemble_en =RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
        "# train the ensemble\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# evaluate the ensemble\n",
        "ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "ensemble_es =RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
        "# train the ensemble\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# evaluate the ensemble\n",
        "ensemble_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ],
      "metadata": {
        "id": "XVSyFvrmtDqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1697182e-781d-4769-8fee-3a2e1b12d2e6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: -0.03485408693137697\n",
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.1695519914577131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE  9\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "# Create the boosting classifier\n",
        "ensemble_en=AdaBoostClassifier(base_estimator=LogisticRegression(),n_estimators=10)\n",
        "# Train the boosting classifier on the training data\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# Evaluate the boosting classifier on the testing data\n",
        "predicted = ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "# Create the boosting classifier\n",
        "ensemble_es=AdaBoostClassifier(base_estimator=LogisticRegression(),n_estimators=10)\n",
        "# Train the boosting classifier on the training data\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# Evaluate the boosting classifier on the testing data\n",
        "predicted = ensemble_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ],
      "metadata": {
        "id": "AESMCErVtMHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32491aa2-f4ab-4bb3-9efa-c334ff60a994"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.5515440584801559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.4809382436142841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 10\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "# Convert data to DMatrix format\n",
        "dtrain_en = xgb.DMatrix(X_en_train, label=y_en_train)\n",
        "dtest_en = xgb.DMatrix(X_en_test, label=y_en_test)\n",
        "\n",
        "# Set XGBoost parameters\n",
        "params = {\n",
        "   'max_depth': 3,\n",
        "   'eta': 0.1,\n",
        "   'objective': 'binary:logistic',\n",
        "   'eval_metric': 'auc'\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "num_rounds = 100\n",
        "bst_en = xgb.train(params, dtrain_en, num_rounds)\n",
        "# Make predictions on the test set\n",
        "predicted = bst_en.predict(dtest_en)\n",
        "# Evaluate the model\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "predicted = np.asarray([round(value) for value in predicted])\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "\n",
        "# Convert data to DMatrix format\n",
        "dtrain_es = xgb.DMatrix(X_es_train, label=y_es_train)\n",
        "dtest_es = xgb.DMatrix(X_es_test, label=y_es_test)\n",
        "\n",
        "# Set XGBoost parameters\n",
        "params = {\n",
        "   'max_depth': 3,\n",
        "   'eta': 0.1,\n",
        "   'objective': 'binary:logistic',\n",
        "   'eval_metric': 'auc'\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "num_rounds = 100\n",
        "bst_es = xgb.train(params, dtrain_es, num_rounds)\n",
        "# Make predictions on the test set\n",
        "predicted = bst_es.predict(dtest_es)\n",
        "# Evaluate the model\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "predicted = np.asarray([round(value) for value in predicted])\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ],
      "metadata": {
        "id": "4XeN8su3tUxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788696ce-e561-4a03-8335-c1959137a043"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Predicted\t-->\t [0.18434063 0.2241836  0.11923677 0.20737405 0.06573689 0.6290595\n",
            " 0.3864707  0.8867527  0.20124248 0.8936915  0.58740264 0.82706803\n",
            " 0.06040424 0.17905107 0.12950625 0.04967542 0.15846328 0.12672687\n",
            " 0.8337534  0.68943125 0.698328   0.23992816 0.58177817 0.7861007\n",
            " 0.19562127 0.42405805 0.04027979 0.16963732 0.20536499 0.17955299\n",
            " 0.8912415  0.32728812 0.18227866 0.2047077  0.03933467 0.4190578\n",
            " 0.13323498 0.8044822  0.24382278 0.2352227  0.17350385 0.8071747\n",
            " 0.7921219  0.10891214 0.13861509 0.13840972 0.6350267  0.1721873\n",
            " 0.13381204 0.9004269  0.25133812 0.7789721  0.52266824 0.14751948\n",
            " 0.10433385 0.25791356 0.18227866 0.9696572  0.9612506  0.4532724\n",
            " 0.30484995 0.8527959  0.14126456 0.14586338 0.32341772 0.80587846\n",
            " 0.35523322 0.7968726  0.07525552 0.14754106 0.1666345  0.1969439\n",
            " 0.06933396 0.1480879  0.96226704 0.80055255 0.30354327 0.50533646\n",
            " 0.8166155  0.27508998 0.692077   0.11307254 0.11236033 0.14126456\n",
            " 0.7544328  0.31237575 0.86004436 0.17212613 0.8451585  0.11637954\n",
            " 0.21387184 0.22688814 0.5792736  0.97752917 0.7808785  0.17579164\n",
            " 0.16830188 0.17320669 0.15615103 0.46083373 0.14793871 0.86095375\n",
            " 0.23138577 0.6559227  0.35561892 0.04932268 0.1613657  0.98294926\n",
            " 0.13092457 0.69877285 0.18823755 0.9593495  0.2906113  0.3858366\n",
            " 0.16830188 0.29901174 0.18566838 0.1803018  0.18796247 0.03492586\n",
            " 0.5335166  0.1506937  0.15750593 0.11381204 0.4256046  0.7891539\n",
            " 0.13840972 0.8069768  0.65665734 0.09903439 0.27122188 0.69908047\n",
            " 0.8612843  0.08902245 0.16168723 0.213188   0.1943856  0.45206732\n",
            " 0.37880725 0.13888563 0.38255507 0.20241947 0.13008904 0.9507925\n",
            " 0.4766272  0.71447116 0.9594861  0.6961748  0.1506937  0.2815219\n",
            " 0.14150158 0.06994639 0.05961164 0.03004993 0.9877247  0.24444337\n",
            " 0.38950217 0.15195662 0.09640709 0.1613657  0.17937167 0.27730337\n",
            " 0.14343072 0.15185761 0.70463425 0.14333701 0.7693813  0.41269436\n",
            " 0.26741046 0.89800686 0.62730414 0.14343072 0.05553556 0.09651671\n",
            " 0.13840972 0.7235791  0.19724694 0.45282882 0.09860162 0.14665276\n",
            " 0.95004237 0.1487009  0.09635707 0.22124682 0.11222237 0.13633156\n",
            " 0.22600818 0.1115066  0.19269294 0.53841937 0.11803237 0.32288694\n",
            " 0.36028284 0.13840972 0.09399162 0.15349853 0.13142192 0.05694878\n",
            " 0.10049771 0.14343072 0.14046326 0.12471719 0.4558279  0.7603371\n",
            " 0.4656842  0.17154144 0.23503624 0.03380511 0.29527873 0.11803237\n",
            " 0.5214247  0.09856579 0.1004523  0.03446879 0.17887223 0.23384473\n",
            " 0.3027572  0.4486737  0.31540653 0.12588514 0.07154106 0.25183427\n",
            " 0.14428797 0.12455441 0.33262983 0.21599095 0.18271442 0.11333422\n",
            " 0.05005267 0.35054395 0.20414254 0.11803237 0.11539567 0.8486856\n",
            " 0.25714648 0.0354102  0.20106684 0.7758151  0.16935728 0.8629734\n",
            " 0.17638361 0.19391443 0.9801185  0.8127549  0.95057505 0.33029118\n",
            " 0.24367443 0.6100631  0.11034705 0.20027791 0.94598585 0.10522263\n",
            " 0.15557805 0.3698702  0.97080946 0.292884   0.18084487 0.96186835\n",
            " 0.10430212 0.18846959 0.24752204 0.14441857 0.14973272 0.14489035\n",
            " 0.18848802 0.7895225  0.46291104 0.34588957 0.6942015  0.9184554\n",
            " 0.990522   0.1613657  0.15640245 0.14010009 0.09922288 0.7873735\n",
            " 0.1926647  0.21079125 0.35281116 0.10785422 0.03243539 0.19348386\n",
            " 0.09903439 0.17448029 0.36018646 0.75655496 0.04317269 0.26944903\n",
            " 0.12673323 0.8671312  0.14126456 0.10785422 0.27109015 0.8305015\n",
            " 0.12753628 0.6601398  0.15490839 0.6615096  0.07270236 0.12198231\n",
            " 0.1625334  0.11690021 0.18642229 0.8171361  0.34856832 0.16726184\n",
            " 0.22008401 0.13840972 0.5991211  0.885294   0.22330524 0.07925031\n",
            " 0.12032651 0.2593833  0.9929557  0.18298711 0.97952384 0.2077224\n",
            " 0.78242177 0.13541268 0.1823154  0.17070623 0.13530974 0.54803705\n",
            " 0.15632617 0.12194806 0.44395217 0.91797614 0.5455297  0.98553294\n",
            " 0.14150158 0.12982005 0.14611593 0.8069033  0.6939688  0.18138638\n",
            " 0.13881963 0.09903439 0.9527818  0.20541044 0.0734759  0.48631155\n",
            " 0.9559589  0.2258379  0.34863302 0.8366887  0.37793323 0.03588017\n",
            " 0.8193051  0.12628649 0.16516294 0.10785422 0.14843684 0.13099375\n",
            " 0.22972216 0.22325623 0.03446879 0.4818747  0.03292526 0.97442406\n",
            " 0.12455441 0.8450707  0.20644356 0.5080707  0.331264   0.518439\n",
            " 0.18138638 0.10187429 0.28593758 0.275337   0.37641382 0.7530846\n",
            " 0.9946814  0.43448937 0.11991709 0.97466904 0.39391473 0.9004269\n",
            " 0.182069   0.22284818 0.26398212 0.1480879  0.13840972 0.13841417\n",
            " 0.14343072 0.9341974  0.10632671 0.3017027  0.17579164 0.32461536\n",
            " 0.33686852 0.47325775 0.9874422  0.95633966 0.2879935  0.1605793\n",
            " 0.11060315 0.14850941 0.3113083  0.2564547 ]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6867715859126834\n",
            "Predicted\t-->\t [0.31635594 0.1898033  0.11337972 0.21160206 0.63945323 0.2529002\n",
            " 0.16504727 0.3320299  0.97907627 0.18189771 0.60843843 0.18515806\n",
            " 0.3976754  0.11636968 0.5835561  0.3596684  0.2731176  0.48833564\n",
            " 0.72679245 0.61950105 0.30532008 0.3149967  0.43896148 0.07569653\n",
            " 0.3058942  0.07172709 0.23343976 0.20989163 0.21474078 0.23720127\n",
            " 0.26284057 0.69785166 0.1609633  0.1604874  0.6089569  0.3717178\n",
            " 0.17876883 0.61228245 0.23733851 0.1400848  0.21999684 0.9175877\n",
            " 0.16664681 0.6166138  0.12064156 0.24377382 0.1773106  0.1722069\n",
            " 0.20574938 0.80438435 0.20453143 0.21247418 0.22177649 0.64206964\n",
            " 0.7678658  0.3675239  0.16515914 0.9486401  0.5039012  0.19567904\n",
            " 0.23127265 0.39259762 0.4502063  0.8241677  0.16404068 0.28128475\n",
            " 0.43012398 0.15012531 0.11420929 0.1571506  0.329637   0.3860348\n",
            " 0.14878148 0.6326986  0.4289626  0.10695504 0.7208982  0.19158888\n",
            " 0.20932682 0.3410274  0.5093087  0.14756474 0.15085134 0.33209503\n",
            " 0.8768736  0.47421074 0.75359935 0.23561287 0.37377954 0.16948576\n",
            " 0.2542463  0.19560058 0.17927517 0.2209267  0.15677029 0.17784289\n",
            " 0.20806147 0.93415904 0.15528148 0.19085589 0.44615623 0.48240754\n",
            " 0.355276   0.39152363 0.18748626 0.19314934 0.23831467 0.7300352\n",
            " 0.7575591  0.14283508 0.42445698 0.6378539  0.5201333  0.23350315\n",
            " 0.36520892 0.1562784  0.1385445  0.2648452  0.8614056  0.3943792\n",
            " 0.29208308 0.15677029 0.40245572 0.21723352 0.21846539 0.5354415\n",
            " 0.20673725 0.68149155 0.16331926 0.7942679  0.20991331 0.14746064\n",
            " 0.37322167 0.30515912 0.24760991 0.2933553  0.26729223 0.14235006\n",
            " 0.22701067 0.24679565 0.16354288 0.23536389 0.5289475  0.2450519\n",
            " 0.6844131  0.22451274 0.37411448 0.126904   0.2539113  0.3654027\n",
            " 0.6298636  0.831892   0.3544756  0.16670646 0.16791831 0.3699078\n",
            " 0.13948631 0.48189366 0.18458407 0.48878953 0.21191558 0.9937641\n",
            " 0.35665777 0.12692021 0.18694222 0.34674898 0.68230927 0.07172709\n",
            " 0.38107046 0.27109075 0.39383116 0.27079597 0.92361075 0.5184913\n",
            " 0.13321826 0.4728236  0.862037   0.26348677 0.3580649  0.8455439\n",
            " 0.18131705 0.886031   0.16659367 0.83434755 0.5127244  0.13376543\n",
            " 0.2499989  0.18986268 0.34324542 0.2679038  0.151337   0.19321314\n",
            " 0.18009919 0.14982732 0.11550308 0.08520692 0.69428885 0.24890116\n",
            " 0.75447774 0.81315184 0.27125168 0.86871904 0.3644439  0.6418566\n",
            " 0.17245299 0.14731996 0.17286894 0.1650872  0.2354487  0.34828782\n",
            " 0.1577006  0.3714006  0.16941436 0.3097081  0.7171399  0.1841593\n",
            " 0.1797113  0.34081668 0.23107438 0.3520755  0.15133095 0.17068225\n",
            " 0.33973816 0.45209956 0.19495861 0.24694645 0.18104146 0.72011364\n",
            " 0.13579895 0.17242645 0.1752375  0.14319159 0.40071493 0.31085855\n",
            " 0.12728612 0.3190828  0.2260586  0.39501962 0.5702539  0.5875937\n",
            " 0.52889717 0.8373555  0.35476157 0.16972025 0.4974132  0.25497556\n",
            " 0.28980404 0.8693738  0.14779067 0.487287   0.70694363 0.16839312\n",
            " 0.39576072 0.31961748 0.08796445 0.6115048  0.2897016  0.25580493\n",
            " 0.13936113 0.3441974  0.46126318 0.1885508  0.36182743 0.1321654\n",
            " 0.16219516 0.18096185 0.18082583 0.14789183 0.07286727 0.36413884\n",
            " 0.28248334 0.16818362 0.45363462 0.25898603 0.47840953 0.8625482\n",
            " 0.763907   0.15719622 0.17452812 0.12822336 0.15957618 0.1945934\n",
            " 0.2998797  0.28200322 0.16878691 0.24229418 0.5202387  0.2841935\n",
            " 0.7609216  0.65383744 0.1622332  0.37113646 0.47254002 0.23403418\n",
            " 0.18216388 0.1476968  0.7349431  0.20916915 0.9660443  0.9013841\n",
            " 0.19853431 0.21666454 0.13044418 0.3439014  0.1737354  0.39989895\n",
            " 0.21914592 0.18690538 0.16479959 0.24752612 0.39593735 0.18269253\n",
            " 0.30545875 0.9768315  0.28580108 0.31119633 0.24160954 0.7094176\n",
            " 0.58363664 0.68916684 0.10011395 0.19724    0.42101705 0.44545448\n",
            " 0.06877536 0.4028589  0.40661976 0.49238467 0.7858802  0.9403935\n",
            " 0.30806854 0.2560898  0.1295813  0.23906924 0.1133381  0.12918858\n",
            " 0.2572416  0.8093348  0.37967795 0.6119148  0.13150637 0.45991164\n",
            " 0.15018587 0.20780414 0.47780532 0.4834177  0.328899   0.14440277\n",
            " 0.5393845  0.15676457 0.1650872  0.14155482 0.19783813 0.21977957\n",
            " 0.17674491 0.16092183 0.72622555 0.44054604 0.21497487 0.25268048\n",
            " 0.3784583  0.19061638 0.12662336 0.16353598 0.9287728  0.32357967\n",
            " 0.19539449 0.8439414  0.12373121 0.5627876  0.579139   0.7243026\n",
            " 0.14104444 0.20281392 0.12008718 0.15544403 0.5206856  0.28079265\n",
            " 0.84570634 0.1650872  0.48822275 0.32326823 0.28555375 0.6543524\n",
            " 0.20449929 0.47351488 0.20888409 0.16972025 0.29462323 0.18225011\n",
            " 0.23390292 0.23165081 0.17807007 0.4786414  0.25229868 0.10657324\n",
            " 0.8639055  0.16310214 0.45981786 0.1497034 ]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.5659540617775988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 11\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "# create the base models,K-NNeighbors, Logistic Regression, Decision Tree, and Support Vector Machine\n",
        "base_models_en = [('dt', DecisionTreeClassifier()), ('svm', SVC()), ('knn', KNeighborsClassifier())]\n",
        "# Create the meta-model\n",
        "meta_model_en = LogisticRegression()\n",
        "# Create the stacking classifier\n",
        "ensemble_en = StackingClassifier(estimators=base_models_en, final_estimator=meta_model_en)\n",
        "# Train the stacking classifier on the training data\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# Evaluate the stacking classifier on the testing data\n",
        "predicted = ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "base_models_es = [('dt', DecisionTreeClassifier()), ('svm', SVC()), ('knn', KNeighborsClassifier())]\n",
        "# Create the meta-model\n",
        "meta_model_es = LogisticRegression()\n",
        "# Create the stacking classifier\n",
        "ensemble_es = StackingClassifier(estimators=base_models_es, final_estimator=meta_model_es)\n",
        "# Train the stacking classifier on the training data\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# Evaluate the stacking classifier on the testing data\n",
        "predicted = ensemble_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)\n"
      ],
      "metadata": {
        "id": "of7SJ3VxtfvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac59228-acd6-43e2-b915-925ebeec6f9d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6784440161868999\n",
            "Predicted\t-->\t [0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.5474376369547796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 12\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
        "mcc_scorer = make_scorer(matthews_corrcoef, sample_weight=None)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "clf = GridSearchCV(estimator=svm.SVC(), param_grid={'C': [2,3,4,5,10], 'kernel': ('linear', 'rbf')}, scoring=mcc_scorer, n_jobs=-1, verbose=1)\n",
        "clf.fit(X_en, Y_en)\n",
        "#Obtaining the best hyper-parameters\n",
        "print(clf.best_params_)\n",
        "#Obtain the best classifier\n",
        "print(clf.best_estimator_)"
      ],
      "metadata": {
        "id": "dceKeU-gJa6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877fbf90-f001-4e93-b13a-6893f410a935"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 5, 'kernel': 'rbf'}\n",
            "SVC(C=5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE 13\n",
        "#- uniform: uniform distribution in: [loc, loc + scale]\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
        "mcc_scorer = make_scorer(matthews_corrcoef, sample_weight=None)\n",
        "from scipy.stats import uniform\n",
        "logistic = LogisticRegression()\n",
        "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
        "clf = RandomizedSearchCV(logistic, distributions, scoring=mcc_scorer, n_jobs=-1, verbose=1)\n",
        "clf.fit(X_es, Y_es)\n",
        "print(clf.best_params_)\n",
        "print(clf.best_estimator_)"
      ],
      "metadata": {
        "id": "eJXzWBGkJsUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b1490d-22f6-49cc-ecc1-f9e82da730b8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.59389905 0.59157711 0.59292577        nan        nan 0.58363735\n",
            " 0.58820572 0.59102109        nan 0.5948746 ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 1.7405868128518316, 'penalty': 'l2'}\n",
            "LogisticRegression(C=1.7405868128518316)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}