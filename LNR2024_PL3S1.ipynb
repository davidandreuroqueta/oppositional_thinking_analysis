{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfu5rYyDl8oM",
        "outputId": "e5a43b28-91db-4ac0-d50d-67cd2b35a719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'pan-clef-2024-oppositional'...\n"
          ]
        }
      ],
      "source": [
        "#Installing the utilities provided by the organizers of the shared task \"\"\n",
        "!git clone https://github.com/dkorenci/pan-clef-2024-oppositional.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdj112eImJzW",
        "metadata": {},
        "outputId": "451f159b-8ff3-4d4b-e426-06056d1effdb"
      },
      "outputs": [],
      "source": [
        "!bash /content/pan-clef-2024-oppositional/generate_requirements.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RULJcH1qRwn",
        "outputId": "2561c3b5-fb9b-4608-ec14-84ec99a9571a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
            "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\dandr\\OneDrive\\1Ciencia de datos\\LNR\\Lab\\oppositional_thinking_analysis\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LBnGu_lCqooY",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "sys.path.append(\"/content/pan-clef-2024-oppositional/\")\n",
        "TRAIN_DATASET_ES=\"/content/drive/MyDrive/LNR/LNR 2024/Dataset-Oppositionl/training/dataset_oppositional/dataset_es_train.json\"\n",
        "TRAIN_DATASET_EN=\"/content/drive/MyDrive/LNR/LNR 2024/Dataset-Oppositionl/training/dataset_oppositional/dataset_en_train.json\"\n",
        "TEST_DATASET_EN =\"/content/drive/MyDrive/LNR/LNR 2024/Dataset-Oppositionl/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\"\n",
        "TEST_DATASET_ES =\"/content/drive/MyDrive/LNR/LNR 2024/Dataset-Oppositionl/test/dataset_oppositional_test_nolabels/dataset_en_official_test_nolabels.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FClav3aLqp7N",
        "metadata": {},
        "outputId": "54e53a23-bd66-4a96-dbbe-17fecbd18cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading official JSON es dataset\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../Dataset-Oppositionl/training/dataset_es_train.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_tools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset_classification\n\u001b[1;32m----> 2\u001b[0m texts_es, labels_es, ids_es \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconspiracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m texts_en, labels_en, ids_en \u001b[38;5;241m=\u001b[39m load_dataset_classification(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, string_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, positive_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconspiracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\dandr\\OneDrive\\1Ciencia de datos\\LNR\\Lab\\oppositional_thinking_analysis\\pan-clef-2024-oppositional\\data_tools\\dataset_loaders.py:38\u001b[0m, in \u001b[0;36mload_dataset_classification\u001b[1;34m(lang, string_labels, positive_class)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset_classification\u001b[39m(lang, string_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, positive_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconspiracy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    Load official .json dataset and convert it to a format suitable for classification.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    :param lang: 'en' or 'es'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    :return: three pandas series: texts, binary classes (1 - positive, 0 - negative), text ids\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# convert to a format suitable for classification\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     texts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries([doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m dataset])\n",
            "File \u001b[1;32mc:\\Users\\dandr\\OneDrive\\1Ciencia de datos\\LNR\\Lab\\oppositional_thinking_analysis\\pan-clef-2024-oppositional\\data_tools\\dataset_loaders.py:24\u001b[0m, in \u001b[0;36mload_dataset_full\u001b[1;34m(lang, format)\u001b[0m\n\u001b[0;32m     22\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m reconstruct_spacy_docs_from_json(fname, lang)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     25\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Dataset-Oppositionl/training/dataset_es_train.json'"
          ]
        }
      ],
      "source": [
        "from data_tools.dataset_loaders import load_dataset_classification\n",
        "texts_es, labels_es, ids_es = load_dataset_classification(\"es\", string_labels=False, positive_class='conspiracy')\n",
        "texts_en, labels_en, ids_en = load_dataset_classification(\"en\", string_labels=False, positive_class='conspiracy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x8jMKgbG8UtM"
      },
      "outputs": [],
      "source": [
        "#Creating a toy representation for the training dataset in both languages.\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(analyzer = 'word', max_features=4000, lowercase=True)\n",
        "X_es = vectorizer.fit_transform(texts_es.tolist())\n",
        "vectorizer.get_feature_names_out()\n",
        "X_es= X_es.toarray()\n",
        "Y_es= np.asarray(labels_es.tolist())\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer = 'word', max_features=4000, lowercase=True)\n",
        "X_en = vectorizer.fit_transform(texts_en.tolist())\n",
        "vectorizer.get_feature_names_out()\n",
        "X_en= X_en.toarray()\n",
        "Y_en= np.asarray(labels_en.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFbeEK6g2fJE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWVdBVtlqq5m",
        "outputId": "4eab4e32-6159-4193-9857-4c5ed05a39d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6158546335768069\n",
            "Predicted\t-->\t [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.5147027940361023\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 1\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "# Here, we are dividing the trainig set in two subsets one for training and other for validation.\n",
        "# In practice, X_en and X_es represent the matrices with text representations from data provided, and Y_en and Y_es represent the corresponding labels of either critical or conspiracy texts.\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "#Using the default parameters\n",
        "clf_en = svm.SVC()\n",
        "clf_en.fit(X_en_train, y_en_train)\n",
        "predicted = clf_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "#Using the default parameters\n",
        "clf_es = svm.SVC()\n",
        "clf_es.fit(X_es_train, y_es_train)\n",
        "predicted = clf_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OljOO7hSrsef",
        "outputId": "09591fab-c6e0-4d18-e2ab-597d114b7f8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6900063416551955\n",
            "Predicted\t-->\t [0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.6607052811439216\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 2\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "#Using the default parameters\n",
        "clf_en = LogisticRegression()\n",
        "clf_en.fit(X_en_train, y_en_train)\n",
        "predicted = clf_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "\n",
        "#Using the default parameters\n",
        "clf_es= LogisticRegression()\n",
        "clf_es.fit(X_es_train, y_es_train)\n",
        "predicted = clf_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxsE7xo9r39B",
        "outputId": "71db34fa-cc71-4716-c575-25325c1bc7a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.5609754043385264\n",
            "Predicted\t-->\t [1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.39356106462973733\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 3\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "clf_en = DecisionTreeClassifier()\n",
        "clf_en.fit(X_en_train, y_en_train)\n",
        "predicted = clf_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "clf_es = DecisionTreeClassifier()\n",
        "clf_es.fit(X_es_train, y_es_train)\n",
        "predicted = clf_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnUhlPNCsAYR",
        "outputId": "cba54e40-602d-486e-87a8-4dc32d24cc83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6559441012749416\n",
            "Predicted\t-->\t [0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.6146430393212517\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 4\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "#Using the default parameters\n",
        "clf_en = MLPClassifier(random_state=1, max_iter=300)\n",
        "clf_en.fit(X_en_train, y_en_train)\n",
        "predicted = clf_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "\n",
        "#Using the default parameters\n",
        "clf_es = MLPClassifier(random_state=1, max_iter=300)\n",
        "clf_es.fit(X_es_train, y_es_train)\n",
        "predicted = clf_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCoPxuxwsLPy",
        "outputId": "ef5ef2d7-6c5f-445c-c308-898a95856f6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6898001532114117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.1695519914577131\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 5\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#Here you can define the training and test sets for the shared task\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "# create the base models, Logistic Regression, Decision Tree, and Support Vector Machine\n",
        "model1 = LogisticRegression()\n",
        "model2 = DecisionTreeClassifier()\n",
        "model3 = SVC()\n",
        "# create the voting ensemble\n",
        "ensemble_en = VotingClassifier(estimators=[('lr', model1), ('dt', model2), ('svm', model3)], voting='hard')\n",
        "# train the ensemble\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# Evaluate the ensemble\n",
        "predicted=ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "\n",
        "# create the base models, Logistic Regression, Decision Tree, and Support Vector Machine\n",
        "model1 = LogisticRegression()\n",
        "model2 = DecisionTreeClassifier()\n",
        "model3 = SVC()\n",
        "# create the voting ensemble\n",
        "ensemble_es = VotingClassifier(estimators=[('lr', model1), ('dt', model2), ('svm', model3)], voting='hard')\n",
        "# train the ensemble\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# Evaluate the ensemble\n",
        "predicted=ensemble_en.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5KeANyPsWX1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1ooMhw7s-Co",
        "outputId": "561b8dff-3357-4054-aed8-42f3e4f2b81b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: -0.03485408693137697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.1695519914577131\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 6\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#Here you can define the training and test sets for the shared task\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "ensemble_en =BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)\n",
        "# train the ensemble\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# evaluate the ensemble\n",
        "ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "ensemble_es =BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)\n",
        "# train the ensemble\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# evaluate the ensemble\n",
        "ensemble_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVSyFvrmtDqV",
        "outputId": "1697182e-781d-4769-8fee-3a2e1b12d2e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: -0.03485408693137697\n",
            "Predicted\t-->\t [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.1695519914577131\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 8\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#Here you can define the training and test sets for the shared task\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "ensemble_en =RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
        "# train the ensemble\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# evaluate the ensemble\n",
        "ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "ensemble_es =RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
        "# train the ensemble\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# evaluate the ensemble\n",
        "ensemble_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AESMCErVtMHq",
        "outputId": "32491aa2-f4ab-4bb3-9efa-c334ff60a994"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.5515440584801559\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.4809382436142841\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE  9\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "# Create the boosting classifier\n",
        "ensemble_en=AdaBoostClassifier(base_estimator=LogisticRegression(),n_estimators=10)\n",
        "# Train the boosting classifier on the training data\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# Evaluate the boosting classifier on the testing data\n",
        "predicted = ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "# Create the boosting classifier\n",
        "ensemble_es=AdaBoostClassifier(base_estimator=LogisticRegression(),n_estimators=10)\n",
        "# Train the boosting classifier on the training data\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# Evaluate the boosting classifier on the testing data\n",
        "predicted = ensemble_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XeN8su3tUxq",
        "outputId": "788696ce-e561-4a03-8335-c1959137a043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Predicted\t-->\t [0.18434063 0.2241836  0.11923677 0.20737405 0.06573689 0.6290595\n",
            " 0.3864707  0.8867527  0.20124248 0.8936915  0.58740264 0.82706803\n",
            " 0.06040424 0.17905107 0.12950625 0.04967542 0.15846328 0.12672687\n",
            " 0.8337534  0.68943125 0.698328   0.23992816 0.58177817 0.7861007\n",
            " 0.19562127 0.42405805 0.04027979 0.16963732 0.20536499 0.17955299\n",
            " 0.8912415  0.32728812 0.18227866 0.2047077  0.03933467 0.4190578\n",
            " 0.13323498 0.8044822  0.24382278 0.2352227  0.17350385 0.8071747\n",
            " 0.7921219  0.10891214 0.13861509 0.13840972 0.6350267  0.1721873\n",
            " 0.13381204 0.9004269  0.25133812 0.7789721  0.52266824 0.14751948\n",
            " 0.10433385 0.25791356 0.18227866 0.9696572  0.9612506  0.4532724\n",
            " 0.30484995 0.8527959  0.14126456 0.14586338 0.32341772 0.80587846\n",
            " 0.35523322 0.7968726  0.07525552 0.14754106 0.1666345  0.1969439\n",
            " 0.06933396 0.1480879  0.96226704 0.80055255 0.30354327 0.50533646\n",
            " 0.8166155  0.27508998 0.692077   0.11307254 0.11236033 0.14126456\n",
            " 0.7544328  0.31237575 0.86004436 0.17212613 0.8451585  0.11637954\n",
            " 0.21387184 0.22688814 0.5792736  0.97752917 0.7808785  0.17579164\n",
            " 0.16830188 0.17320669 0.15615103 0.46083373 0.14793871 0.86095375\n",
            " 0.23138577 0.6559227  0.35561892 0.04932268 0.1613657  0.98294926\n",
            " 0.13092457 0.69877285 0.18823755 0.9593495  0.2906113  0.3858366\n",
            " 0.16830188 0.29901174 0.18566838 0.1803018  0.18796247 0.03492586\n",
            " 0.5335166  0.1506937  0.15750593 0.11381204 0.4256046  0.7891539\n",
            " 0.13840972 0.8069768  0.65665734 0.09903439 0.27122188 0.69908047\n",
            " 0.8612843  0.08902245 0.16168723 0.213188   0.1943856  0.45206732\n",
            " 0.37880725 0.13888563 0.38255507 0.20241947 0.13008904 0.9507925\n",
            " 0.4766272  0.71447116 0.9594861  0.6961748  0.1506937  0.2815219\n",
            " 0.14150158 0.06994639 0.05961164 0.03004993 0.9877247  0.24444337\n",
            " 0.38950217 0.15195662 0.09640709 0.1613657  0.17937167 0.27730337\n",
            " 0.14343072 0.15185761 0.70463425 0.14333701 0.7693813  0.41269436\n",
            " 0.26741046 0.89800686 0.62730414 0.14343072 0.05553556 0.09651671\n",
            " 0.13840972 0.7235791  0.19724694 0.45282882 0.09860162 0.14665276\n",
            " 0.95004237 0.1487009  0.09635707 0.22124682 0.11222237 0.13633156\n",
            " 0.22600818 0.1115066  0.19269294 0.53841937 0.11803237 0.32288694\n",
            " 0.36028284 0.13840972 0.09399162 0.15349853 0.13142192 0.05694878\n",
            " 0.10049771 0.14343072 0.14046326 0.12471719 0.4558279  0.7603371\n",
            " 0.4656842  0.17154144 0.23503624 0.03380511 0.29527873 0.11803237\n",
            " 0.5214247  0.09856579 0.1004523  0.03446879 0.17887223 0.23384473\n",
            " 0.3027572  0.4486737  0.31540653 0.12588514 0.07154106 0.25183427\n",
            " 0.14428797 0.12455441 0.33262983 0.21599095 0.18271442 0.11333422\n",
            " 0.05005267 0.35054395 0.20414254 0.11803237 0.11539567 0.8486856\n",
            " 0.25714648 0.0354102  0.20106684 0.7758151  0.16935728 0.8629734\n",
            " 0.17638361 0.19391443 0.9801185  0.8127549  0.95057505 0.33029118\n",
            " 0.24367443 0.6100631  0.11034705 0.20027791 0.94598585 0.10522263\n",
            " 0.15557805 0.3698702  0.97080946 0.292884   0.18084487 0.96186835\n",
            " 0.10430212 0.18846959 0.24752204 0.14441857 0.14973272 0.14489035\n",
            " 0.18848802 0.7895225  0.46291104 0.34588957 0.6942015  0.9184554\n",
            " 0.990522   0.1613657  0.15640245 0.14010009 0.09922288 0.7873735\n",
            " 0.1926647  0.21079125 0.35281116 0.10785422 0.03243539 0.19348386\n",
            " 0.09903439 0.17448029 0.36018646 0.75655496 0.04317269 0.26944903\n",
            " 0.12673323 0.8671312  0.14126456 0.10785422 0.27109015 0.8305015\n",
            " 0.12753628 0.6601398  0.15490839 0.6615096  0.07270236 0.12198231\n",
            " 0.1625334  0.11690021 0.18642229 0.8171361  0.34856832 0.16726184\n",
            " 0.22008401 0.13840972 0.5991211  0.885294   0.22330524 0.07925031\n",
            " 0.12032651 0.2593833  0.9929557  0.18298711 0.97952384 0.2077224\n",
            " 0.78242177 0.13541268 0.1823154  0.17070623 0.13530974 0.54803705\n",
            " 0.15632617 0.12194806 0.44395217 0.91797614 0.5455297  0.98553294\n",
            " 0.14150158 0.12982005 0.14611593 0.8069033  0.6939688  0.18138638\n",
            " 0.13881963 0.09903439 0.9527818  0.20541044 0.0734759  0.48631155\n",
            " 0.9559589  0.2258379  0.34863302 0.8366887  0.37793323 0.03588017\n",
            " 0.8193051  0.12628649 0.16516294 0.10785422 0.14843684 0.13099375\n",
            " 0.22972216 0.22325623 0.03446879 0.4818747  0.03292526 0.97442406\n",
            " 0.12455441 0.8450707  0.20644356 0.5080707  0.331264   0.518439\n",
            " 0.18138638 0.10187429 0.28593758 0.275337   0.37641382 0.7530846\n",
            " 0.9946814  0.43448937 0.11991709 0.97466904 0.39391473 0.9004269\n",
            " 0.182069   0.22284818 0.26398212 0.1480879  0.13840972 0.13841417\n",
            " 0.14343072 0.9341974  0.10632671 0.3017027  0.17579164 0.32461536\n",
            " 0.33686852 0.47325775 0.9874422  0.95633966 0.2879935  0.1605793\n",
            " 0.11060315 0.14850941 0.3113083  0.2564547 ]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6867715859126834\n",
            "Predicted\t-->\t [0.31635594 0.1898033  0.11337972 0.21160206 0.63945323 0.2529002\n",
            " 0.16504727 0.3320299  0.97907627 0.18189771 0.60843843 0.18515806\n",
            " 0.3976754  0.11636968 0.5835561  0.3596684  0.2731176  0.48833564\n",
            " 0.72679245 0.61950105 0.30532008 0.3149967  0.43896148 0.07569653\n",
            " 0.3058942  0.07172709 0.23343976 0.20989163 0.21474078 0.23720127\n",
            " 0.26284057 0.69785166 0.1609633  0.1604874  0.6089569  0.3717178\n",
            " 0.17876883 0.61228245 0.23733851 0.1400848  0.21999684 0.9175877\n",
            " 0.16664681 0.6166138  0.12064156 0.24377382 0.1773106  0.1722069\n",
            " 0.20574938 0.80438435 0.20453143 0.21247418 0.22177649 0.64206964\n",
            " 0.7678658  0.3675239  0.16515914 0.9486401  0.5039012  0.19567904\n",
            " 0.23127265 0.39259762 0.4502063  0.8241677  0.16404068 0.28128475\n",
            " 0.43012398 0.15012531 0.11420929 0.1571506  0.329637   0.3860348\n",
            " 0.14878148 0.6326986  0.4289626  0.10695504 0.7208982  0.19158888\n",
            " 0.20932682 0.3410274  0.5093087  0.14756474 0.15085134 0.33209503\n",
            " 0.8768736  0.47421074 0.75359935 0.23561287 0.37377954 0.16948576\n",
            " 0.2542463  0.19560058 0.17927517 0.2209267  0.15677029 0.17784289\n",
            " 0.20806147 0.93415904 0.15528148 0.19085589 0.44615623 0.48240754\n",
            " 0.355276   0.39152363 0.18748626 0.19314934 0.23831467 0.7300352\n",
            " 0.7575591  0.14283508 0.42445698 0.6378539  0.5201333  0.23350315\n",
            " 0.36520892 0.1562784  0.1385445  0.2648452  0.8614056  0.3943792\n",
            " 0.29208308 0.15677029 0.40245572 0.21723352 0.21846539 0.5354415\n",
            " 0.20673725 0.68149155 0.16331926 0.7942679  0.20991331 0.14746064\n",
            " 0.37322167 0.30515912 0.24760991 0.2933553  0.26729223 0.14235006\n",
            " 0.22701067 0.24679565 0.16354288 0.23536389 0.5289475  0.2450519\n",
            " 0.6844131  0.22451274 0.37411448 0.126904   0.2539113  0.3654027\n",
            " 0.6298636  0.831892   0.3544756  0.16670646 0.16791831 0.3699078\n",
            " 0.13948631 0.48189366 0.18458407 0.48878953 0.21191558 0.9937641\n",
            " 0.35665777 0.12692021 0.18694222 0.34674898 0.68230927 0.07172709\n",
            " 0.38107046 0.27109075 0.39383116 0.27079597 0.92361075 0.5184913\n",
            " 0.13321826 0.4728236  0.862037   0.26348677 0.3580649  0.8455439\n",
            " 0.18131705 0.886031   0.16659367 0.83434755 0.5127244  0.13376543\n",
            " 0.2499989  0.18986268 0.34324542 0.2679038  0.151337   0.19321314\n",
            " 0.18009919 0.14982732 0.11550308 0.08520692 0.69428885 0.24890116\n",
            " 0.75447774 0.81315184 0.27125168 0.86871904 0.3644439  0.6418566\n",
            " 0.17245299 0.14731996 0.17286894 0.1650872  0.2354487  0.34828782\n",
            " 0.1577006  0.3714006  0.16941436 0.3097081  0.7171399  0.1841593\n",
            " 0.1797113  0.34081668 0.23107438 0.3520755  0.15133095 0.17068225\n",
            " 0.33973816 0.45209956 0.19495861 0.24694645 0.18104146 0.72011364\n",
            " 0.13579895 0.17242645 0.1752375  0.14319159 0.40071493 0.31085855\n",
            " 0.12728612 0.3190828  0.2260586  0.39501962 0.5702539  0.5875937\n",
            " 0.52889717 0.8373555  0.35476157 0.16972025 0.4974132  0.25497556\n",
            " 0.28980404 0.8693738  0.14779067 0.487287   0.70694363 0.16839312\n",
            " 0.39576072 0.31961748 0.08796445 0.6115048  0.2897016  0.25580493\n",
            " 0.13936113 0.3441974  0.46126318 0.1885508  0.36182743 0.1321654\n",
            " 0.16219516 0.18096185 0.18082583 0.14789183 0.07286727 0.36413884\n",
            " 0.28248334 0.16818362 0.45363462 0.25898603 0.47840953 0.8625482\n",
            " 0.763907   0.15719622 0.17452812 0.12822336 0.15957618 0.1945934\n",
            " 0.2998797  0.28200322 0.16878691 0.24229418 0.5202387  0.2841935\n",
            " 0.7609216  0.65383744 0.1622332  0.37113646 0.47254002 0.23403418\n",
            " 0.18216388 0.1476968  0.7349431  0.20916915 0.9660443  0.9013841\n",
            " 0.19853431 0.21666454 0.13044418 0.3439014  0.1737354  0.39989895\n",
            " 0.21914592 0.18690538 0.16479959 0.24752612 0.39593735 0.18269253\n",
            " 0.30545875 0.9768315  0.28580108 0.31119633 0.24160954 0.7094176\n",
            " 0.58363664 0.68916684 0.10011395 0.19724    0.42101705 0.44545448\n",
            " 0.06877536 0.4028589  0.40661976 0.49238467 0.7858802  0.9403935\n",
            " 0.30806854 0.2560898  0.1295813  0.23906924 0.1133381  0.12918858\n",
            " 0.2572416  0.8093348  0.37967795 0.6119148  0.13150637 0.45991164\n",
            " 0.15018587 0.20780414 0.47780532 0.4834177  0.328899   0.14440277\n",
            " 0.5393845  0.15676457 0.1650872  0.14155482 0.19783813 0.21977957\n",
            " 0.17674491 0.16092183 0.72622555 0.44054604 0.21497487 0.25268048\n",
            " 0.3784583  0.19061638 0.12662336 0.16353598 0.9287728  0.32357967\n",
            " 0.19539449 0.8439414  0.12373121 0.5627876  0.579139   0.7243026\n",
            " 0.14104444 0.20281392 0.12008718 0.15544403 0.5206856  0.28079265\n",
            " 0.84570634 0.1650872  0.48822275 0.32326823 0.28555375 0.6543524\n",
            " 0.20449929 0.47351488 0.20888409 0.16972025 0.29462323 0.18225011\n",
            " 0.23390292 0.23165081 0.17807007 0.4786414  0.25229868 0.10657324\n",
            " 0.8639055  0.16310214 0.45981786 0.1497034 ]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.5659540617775988\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 10\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "# Convert data to DMatrix format\n",
        "dtrain_en = xgb.DMatrix(X_en_train, label=y_en_train)\n",
        "dtest_en = xgb.DMatrix(X_en_test, label=y_en_test)\n",
        "\n",
        "# Set XGBoost parameters\n",
        "params = {\n",
        "   'max_depth': 3,\n",
        "   'eta': 0.1,\n",
        "   'objective': 'binary:logistic',\n",
        "   'eval_metric': 'auc'\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "num_rounds = 100\n",
        "bst_en = xgb.train(params, dtrain_en, num_rounds)\n",
        "# Make predictions on the test set\n",
        "predicted = bst_en.predict(dtest_en)\n",
        "# Evaluate the model\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "predicted = np.asarray([round(value) for value in predicted])\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "\n",
        "# Convert data to DMatrix format\n",
        "dtrain_es = xgb.DMatrix(X_es_train, label=y_es_train)\n",
        "dtest_es = xgb.DMatrix(X_es_test, label=y_es_test)\n",
        "\n",
        "# Set XGBoost parameters\n",
        "params = {\n",
        "   'max_depth': 3,\n",
        "   'eta': 0.1,\n",
        "   'objective': 'binary:logistic',\n",
        "   'eval_metric': 'auc'\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "num_rounds = 100\n",
        "bst_es = xgb.train(params, dtrain_es, num_rounds)\n",
        "# Make predictions on the test set\n",
        "predicted = bst_es.predict(dtest_es)\n",
        "# Evaluate the model\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "predicted = np.asarray([round(value) for value in predicted])\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of7SJ3VxtfvU",
        "outputId": "7ac59228-acd6-43e2-b915-925ebeec6f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted\t-->\t [0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0]\n",
            "MCC for English: 0.6784440161868999\n",
            "Predicted\t-->\t [0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0]\n",
            "GroundTruth\t-->\t [0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1]\n",
            "MCC for Spanish: 0.5474376369547796\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 11\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = train_test_split(X_en, Y_en, test_size=0.1, random_state=1234)\n",
        "X_es_train, X_es_test, y_es_train, y_es_test = train_test_split(X_es, Y_es, test_size=0.1, random_state=1234)\n",
        "\n",
        "# create the base models,K-NNeighbors, Logistic Regression, Decision Tree, and Support Vector Machine\n",
        "base_models_en = [('dt', DecisionTreeClassifier()), ('svm', SVC()), ('knn', KNeighborsClassifier())]\n",
        "# Create the meta-model\n",
        "meta_model_en = LogisticRegression()\n",
        "# Create the stacking classifier\n",
        "ensemble_en = StackingClassifier(estimators=base_models_en, final_estimator=meta_model_en)\n",
        "# Train the stacking classifier on the training data\n",
        "ensemble_en.fit(X_en_train, y_en_train)\n",
        "# Evaluate the stacking classifier on the testing data\n",
        "predicted = ensemble_en.predict(X_en_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_en_test)\n",
        "quality=matthews_corrcoef(y_en_test, predicted)\n",
        "print(\"MCC for English:\", quality)\n",
        "\n",
        "base_models_es = [('dt', DecisionTreeClassifier()), ('svm', SVC()), ('knn', KNeighborsClassifier())]\n",
        "# Create the meta-model\n",
        "meta_model_es = LogisticRegression()\n",
        "# Create the stacking classifier\n",
        "ensemble_es = StackingClassifier(estimators=base_models_es, final_estimator=meta_model_es)\n",
        "# Train the stacking classifier on the training data\n",
        "ensemble_es.fit(X_es_train, y_es_train)\n",
        "# Evaluate the stacking classifier on the testing data\n",
        "predicted = ensemble_es.predict(X_es_test)\n",
        "print(\"Predicted\\t-->\\t\", predicted)\n",
        "print(\"GroundTruth\\t-->\\t\", y_es_test)\n",
        "quality=matthews_corrcoef(y_es_test, predicted)\n",
        "print(\"MCC for Spanish:\", quality)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dceKeU-gJa6N",
        "outputId": "877fbf90-f001-4e93-b13a-6893f410a935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 5, 'kernel': 'rbf'}\n",
            "SVC(C=5)\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 12\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
        "mcc_scorer = make_scorer(matthews_corrcoef, sample_weight=None)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "clf = GridSearchCV(estimator=svm.SVC(), param_grid={'C': [2,3,4,5,10], 'kernel': ('linear', 'rbf')}, scoring=mcc_scorer, n_jobs=-1, verbose=1)\n",
        "clf.fit(X_en, Y_en)\n",
        "#Obtaining the best hyper-parameters\n",
        "print(clf.best_params_)\n",
        "#Obtain the best classifier\n",
        "print(clf.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJXzWBGkJsUn",
        "outputId": "94b1490d-22f6-49cc-ecc1-f9e82da730b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.59389905 0.59157711 0.59292577        nan        nan 0.58363735\n",
            " 0.58820572 0.59102109        nan 0.5948746 ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 1.7405868128518316, 'penalty': 'l2'}\n",
            "LogisticRegression(C=1.7405868128518316)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE 13\n",
        "#- uniform: uniform distribution in: [loc, loc + scale]\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import matthews_corrcoef, make_scorer\n",
        "mcc_scorer = make_scorer(matthews_corrcoef, sample_weight=None)\n",
        "from scipy.stats import uniform\n",
        "logistic = LogisticRegression()\n",
        "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
        "clf = RandomizedSearchCV(logistic, distributions, scoring=mcc_scorer, n_jobs=-1, verbose=1)\n",
        "clf.fit(X_es, Y_es)\n",
        "print(clf.best_params_)\n",
        "print(clf.best_estimator_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
