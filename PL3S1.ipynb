{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import DebertaTokenizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim.downloader as api # works with scipy==1.10.1\n",
    "import string\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"pan-clef-2024-oppositional/\")\n",
    "TRAIN_DATASET_ES=\"Dataset-Oppositional/training/dataset_es_train.json\"\n",
    "TRAIN_DATASET_EN=\"Dataset-Oppositional/training/dataset_en_train.json\"\n",
    "TEST_DATASET_EN =\"Dataset-Oppositional/test/dataset_en_official_test_nolabels.json\"\n",
    "TEST_DATASET_ES =\"Dataset-Oppositional/test/dataset_en_official_test_nolabels.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading official JSON es dataset\n",
      "Loading official JSON en dataset\n"
     ]
    }
   ],
   "source": [
    "from data_tools.dataset_loaders import load_dataset_classification\n",
    "texts_es, labels_es, ids_es = load_dataset_classification(\"es\", string_labels=False, positive_class='conspiracy')\n",
    "texts_en, labels_en, ids_en = load_dataset_classification(\"en\", string_labels=False, positive_class='conspiracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a pandas dataframe: 'id': ids_es, 'text': texts_es, 'label': labels_es, 'y': labels_es\n",
    "df_es = pd.DataFrame({'id': ids_es, 'text': texts_es, 'label': labels_es})\n",
    "df_en = pd.DataFrame({'id': ids_en, 'text': texts_en, 'label': labels_en})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2807</td>\n",
       "      <td>Fallo en Matrix 08/02/2022 Hoy el señor Joan R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3054</td>\n",
       "      <td>Siento ya tdas las vacunas vienen contaminadas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268</td>\n",
       "      <td>Veo que curiosamente te autoproclamados interl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2669</td>\n",
       "      <td>[ Documental ] Vacunas : Una inyección en la o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3205</td>\n",
       "      <td>Una sugerencia para los que se han vacunado y ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>1056</td>\n",
       "      <td>DR . ROBERT MALONE . Co - Inventor de las tecn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>861</td>\n",
       "      <td>Una pregunta ? La vacuna también provoca hipot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>5248</td>\n",
       "      <td>ERIC CLAPTON : El famoso guitarrista cuenta co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>1328</td>\n",
       "      <td>No es médico , no es científico , no es biólog...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>124</td>\n",
       "      <td>Hola , aporto prueba de curación : Mi padre , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  label\n",
       "0     2807  Fallo en Matrix 08/02/2022 Hoy el señor Joan R...      0\n",
       "1     3054  Siento ya tdas las vacunas vienen contaminadas...      0\n",
       "2      268  Veo que curiosamente te autoproclamados interl...      1\n",
       "3     2669  [ Documental ] Vacunas : Una inyección en la o...      0\n",
       "4     3205  Una sugerencia para los que se han vacunado y ...      1\n",
       "...    ...                                                ...    ...\n",
       "3995  1056  DR . ROBERT MALONE . Co - Inventor de las tecn...      0\n",
       "3996   861  Una pregunta ? La vacuna también provoca hipot...      0\n",
       "3997  5248  ERIC CLAPTON : El famoso guitarrista cuenta co...      0\n",
       "3998  1328  No es médico , no es científico , no es biólog...      1\n",
       "3999   124  Hola , aporto prueba de curación : Mi padre , ...      0\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation models\n",
    "\n",
    "- Traditional form: **Latent Semantic Analysis (LSA)**. LSA is a technique used in NLP and information retrieval to\n",
    "analyze the relationships between a set of documents and the\n",
    "terms they contain. It's primarily employed for text classification,\n",
    "clustering, and information retrieval tasks.\n",
    "\n",
    "- Static and contextual: **fasttext-wiki-news-subwords-300** for english text. Due to this model was trained only with english text, to make better representation in spanish, we need another model. For spanish we are going to use a model similar trained with spanish text that is trained on Common Crawl and Wikipedia using fastText. This models was trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "\n",
    "- Contextual word embedding: **DeBERTa**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fasttext-wiki-news-subwords-300', {'num_records': 999999, 'file_size': 1005007116, 'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py', 'license': 'https://creativecommons.org/licenses/by-sa/3.0/', 'parameters': {'dimension': 300}, 'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).', 'read_more': ['https://fasttext.cc/docs/en/english-vectors.html', 'https://arxiv.org/abs/1712.09405', 'https://arxiv.org/abs/1607.01759'], 'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af', 'file_name': 'fasttext-wiki-news-subwords-300.gz', 'parts': 1})\n",
      "('conceptnet-numberbatch-17-06-300', {'num_records': 1917247, 'file_size': 1225497562, 'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py', 'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt', 'parameters': {'dimension': 300}, 'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.', 'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972', 'https://github.com/commonsense/conceptnet-numberbatch', 'http://conceptnet.io/'], 'checksum': 'fd642d457adcd0ea94da0cd21b150847', 'file_name': 'conceptnet-numberbatch-17-06-300.gz', 'parts': 1})\n",
      "('word2vec-ruscorpora-300', {'num_records': 184973, 'file_size': 208427381, 'base_dataset': 'Russian National Corpus (about 250M words)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py', 'license': 'https://creativecommons.org/licenses/by/4.0/deed.en', 'parameters': {'dimension': 300, 'window_size': 10}, 'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.', 'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS', 'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models', 'http://rusvectores.org/en/', 'https://github.com/RaRe-Technologies/gensim-data/issues/3'], 'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4', 'file_name': 'word2vec-ruscorpora-300.gz', 'parts': 1})\n",
      "('word2vec-google-news-300', {'num_records': 3000000, 'file_size': 1743563840, 'base_dataset': 'Google News (about 100 billion words)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py', 'license': 'not found', 'parameters': {'dimension': 300}, 'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\", 'read_more': ['https://code.google.com/archive/p/word2vec/', 'https://arxiv.org/abs/1301.3781', 'https://arxiv.org/abs/1310.4546', 'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'], 'checksum': 'a5e5354d40acb95f9ec66d5977d140ef', 'file_name': 'word2vec-google-news-300.gz', 'parts': 1})\n",
      "('glove-wiki-gigaword-50', {'num_records': 400000, 'file_size': 69182535, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 50}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813', 'file_name': 'glove-wiki-gigaword-50.gz', 'parts': 1})\n",
      "('glove-wiki-gigaword-100', {'num_records': 400000, 'file_size': 134300434, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 100}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '40ec481866001177b8cd4cb0df92924f', 'file_name': 'glove-wiki-gigaword-100.gz', 'parts': 1})\n",
      "('glove-wiki-gigaword-200', {'num_records': 400000, 'file_size': 264336934, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 200}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '59652db361b7a87ee73834a6c391dfc1', 'file_name': 'glove-wiki-gigaword-200.gz', 'parts': 1})\n",
      "('glove-wiki-gigaword-300', {'num_records': 400000, 'file_size': 394362229, 'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 300}, 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '29e9329ac2241937d55b852e8284e89b', 'file_name': 'glove-wiki-gigaword-300.gz', 'parts': 1})\n",
      "('glove-twitter-25', {'num_records': 1193514, 'file_size': 109885004, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 25}, 'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': '50db0211d7e7a2dcd362c6b774762793', 'file_name': 'glove-twitter-25.gz', 'parts': 1})\n",
      "('glove-twitter-50', {'num_records': 1193514, 'file_size': 209216938, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 50}, 'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'c168f18641f8c8a00fe30984c4799b2b', 'file_name': 'glove-twitter-50.gz', 'parts': 1})\n",
      "('glove-twitter-100', {'num_records': 1193514, 'file_size': 405932991, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 100}, 'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'b04f7bed38756d64cf55b58ce7e97b15', 'file_name': 'glove-twitter-100.gz', 'parts': 1})\n",
      "('glove-twitter-200', {'num_records': 1193514, 'file_size': 795373100, 'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)', 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py', 'license': 'http://opendatacommons.org/licenses/pddl/', 'parameters': {'dimension': 200}, 'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).', 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.', 'read_more': ['https://nlp.stanford.edu/projects/glove/', 'https://nlp.stanford.edu/pubs/glove.pdf'], 'checksum': 'e52e8392d1860b95d5308a525817d8f9', 'file_name': 'glove-twitter-200.gz', 'parts': 1})\n",
      "('__testing_word2vec-matrix-synopsis', {'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.', 'parameters': {'dimensions': 50}, 'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.', 'read_more': [], 'checksum': '534dcb8b56a360977a269b7bfc62d124', 'file_name': '__testing_word2vec-matrix-synopsis.gz', 'parts': 1})\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api # works with scipy==1.10.1\n",
    "info = api.info()\n",
    "for item in info[\"models\"].items():\n",
    "    print(item)\n",
    "    \n",
    "word_vectors = api.load('fasttext-wiki-news-subwords-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dandr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dandr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text: str,\n",
    "                    stop_words=stopwords.words('english')):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords, punctuation marks, and symbols\n",
    "    tokens = [w for w in tokens\n",
    "              if w not in stop_words\n",
    "              and w not in string.punctuation\n",
    "              and w.isalnum()]\n",
    "    return tokens\n",
    "\n",
    "# Function to calculate average vector representation of a document\n",
    "def document_vector(tokens: list[str],\n",
    "                    word_vectors: KeyedVectors):\n",
    "    # Initialize empty vector\n",
    "    document_vector = np.zeros((300,))\n",
    "    # Count number of words in document\n",
    "    words_count = 0\n",
    "    # Iterate over each word in the document\n",
    "    for word in tokens:\n",
    "        if word in word_vectors.key_to_index:\n",
    "            # Add word vector to document vector\n",
    "            document_vector += word_vectors[word]\n",
    "            words_count += 1\n",
    "    # Average the document vector\n",
    "    if words_count != 0:\n",
    "        document_vector /= words_count\n",
    "    return document_vector\n",
    "\n",
    "# Function to create LSA features\n",
    "def lsa_embedding(train_texts: pd.DataFrame,\n",
    "                  val_texts: pd.DataFrame,\n",
    "                  n_components: int=100,\n",
    "                  process_text: bool=True):\n",
    "    \n",
    "    if preprocess_text:\n",
    "        # Preprocess text\n",
    "        train_texts['text'] = train_texts['text'].apply(preprocess_text)\n",
    "        val_texts['text'] = val_texts['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Create TF-IDF vectors\n",
    "    tfidf_vectorizer = TfidfVectorizer(binary=False,\n",
    "                                       ngram_range=(3, 3),\n",
    "                                       analyzer='char')\n",
    "    train_tfidf = tfidf_vectorizer.fit_transform(train_texts['text'])\n",
    "    val_tfidf = tfidf_vectorizer.transform(val_texts['text'])\n",
    "    \n",
    "    # Apply SVD\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    train_lsa = svd.fit_transform(train_tfidf)\n",
    "    val_lsa = svd.transform(val_tfidf)\n",
    "    \n",
    "    # Add LSA embeddings to the dataframes\n",
    "    train_texts['lsa_embedding'] = list(train_lsa)\n",
    "    val_texts['lsa_embedding'] = list(val_lsa)\n",
    "    \n",
    "    return train_lsa, val_lsa\n",
    "\n",
    "\n",
    "def fasttext_embedding(train_texts: pd.DataFrame,\n",
    "                       val_texts: pd.DataFrame):\n",
    "    \n",
    "    if preprocess_text:\n",
    "        # Preprocess text\n",
    "        train_texts['text'] = train_texts['text'].apply(preprocess_text)\n",
    "        val_texts['text'] = val_texts['text'].apply(preprocess_text)\n",
    "    \n",
    "    word_vectors = api.load('fasttext-wiki-news-subwords-300')\n",
    "    \n",
    "    # TRAIN TEXTS\n",
    "    # Create dictionary to store document vectors\n",
    "    document_vectors = {}\n",
    "\n",
    "    # Iterate over each document in the dataset\n",
    "    for index, row in train_texts.iterrows():\n",
    "        # Preprocess text\n",
    "        tokens = preprocess_text(row['text'])\n",
    "        # Calculate document vector\n",
    "        doc_vector = document_vector(tokens, word_vectors)\n",
    "        # Store document vector in dictionary\n",
    "        document_vectors[index] = doc_vector\n",
    "    \n",
    "    # Add document vectors as a new column in the DataFrame\n",
    "    train_texts['fasttext_embeeding'] = document_vectors\n",
    "    \n",
    "    # VAL TEXTS\n",
    "    # Create dictionary to store document vectors\n",
    "    document_vectors = {}\n",
    "\n",
    "    # Iterate over each document in the dataset\n",
    "    for index, row in val_texts.iterrows():\n",
    "        # Preprocess text\n",
    "        tokens = preprocess_text(row['text'])\n",
    "        # Calculate document vector\n",
    "        doc_vector = document_vector(tokens, word_vectors)\n",
    "        # Store document vector in dictionary\n",
    "        document_vectors[index] = doc_vector\n",
    "    \n",
    "    # Add document vectors as a new column in the DataFrame\n",
    "    val_texts['fasttext_embeeding'] = document_vectors \n",
    "    \n",
    "    return train_texts, val_texts\n",
    "\n",
    "\n",
    "def roberta_embedding(train_texts: pd.DataFrame,\n",
    "                      val_texts: pd.DataFrame,\n",
    "                      lang: str='en'):\n",
    "    # Load DeBERTa model and tokenizer\n",
    "    if lang == 'es':\n",
    "        model = RobertaModel.from_pretrained('bertin-project/bertin-roberta-base-spanish')\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('bertin-project/bertin-roberta-base-spanish')\n",
    "    elif lang == 'en':\n",
    "        model = RobertaModel.from_pretrained('roberta-base')\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid language. Expected 'en' or 'es'.\")\n",
    "    \n",
    "    # Function to calculate DeBERTa embeddings\n",
    "    def calculate_embedding(text):\n",
    "        # Tokenize text and convert to tensor\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        # Get model output\n",
    "        outputs = model(**inputs)\n",
    "        # Get embeddings from the last hidden state\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        # Average embeddings over sequence length to get a single vector\n",
    "        doc_vector = embeddings.mean(dim=1).detach().numpy()\n",
    "        return doc_vector\n",
    "\n",
    "    # Calculate DeBERTa embeddings for train_texts and val_texts\n",
    "    train_texts['roberta_embedding'] = train_texts['text'] \\\n",
    "                                            .apply(calculate_embedding)\n",
    "    val_texts['roberta_embedding'] = val_texts['text'] \\\n",
    "                                            .apply(calculate_embedding)\n",
    "\n",
    "    return train_texts, val_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_embedings(train: pd.DataFrame,\n",
    "                  val: pd.DataFrame,\n",
    "                  lang: str):\n",
    "    \n",
    "    train, val = lsa_embedding(train, val)\n",
    "    train, val = fasttext_embedding(train, val)\n",
    "    train, val = roberta_embedding(train, val, lang=lang)\n",
    "    return train, val\n",
    "\n",
    "# Split df_en in train and val\n",
    "train_en, val_en = train_test_split(df_en, test_size=0.2, random_state=50)\n",
    "train_es, val_es = train_test_split(df_es, test_size=0.2, random_state=50)\n",
    "\n",
    "train_en, val_en = all_embedings(train_en, val_en, 'en')\n",
    "train_es, val_es = all_embedings(train_es, val_es, 'es')\n",
    "\n",
    "# Save train_en, val_en, train_es, val_es\n",
    "train_en.to_csv('train_en.csv', index=False, sep=';', encoding='utf-8')\n",
    "val_en.to_csv('val_en.csv', index=False, sep=';', encoding='utf-8')\n",
    "train_es.to_csv('train_es.csv', index=False, sep=';', encoding='utf-8')\n",
    "val_es.to_csv('val_es.csv', index=False, sep=';', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
